{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed34250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bb0b83",
   "metadata": {},
   "source": [
    "### Create SparkSession on cluster with default Hadoop configurations along with application name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c01b6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/05 02:45:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master('local') \\\n",
    "    .appName('ann_banknote') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85514393",
   "metadata": {},
   "source": [
    "### Read the dataset & check the schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5f00e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.csv('data_banknote_authentication.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2b69825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- feature_1: string (nullable = true)\n",
      " |-- feature_2: string (nullable = true)\n",
      " |-- feature_3: string (nullable = true)\n",
      " |-- feature_4: string (nullable = true)\n",
      " |-- Class : string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9436eafa",
   "metadata": {},
   "source": [
    "### Change the schemas from string to double (8-byte double-precision floating point numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da7b1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dataset.columns:\n",
    "    dataset = dataset.withColumn(col,dataset[col].cast('double'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa2454",
   "metadata": {},
   "source": [
    "### Check number of NULL values for all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a135bc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+------+\n",
      "|feature_1|feature_2|feature_3|feature_4|Class |\n",
      "+---------+---------+---------+---------+------+\n",
      "|        1|        1|        1|        0|     0|\n",
      "+---------+---------+---------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, count, isnull\n",
    "dataset.select([count(when(isnull(col), col)).alias(col) for col in dataset.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca82cc3",
   "metadata": {},
   "source": [
    "### Impute NULL values with column's mean & check if it's transformed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4dfed93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "input_cols = ['feature_1', 'feature_2', 'feature_3', 'feature_4']\n",
    "dataset = Imputer(strategy='mean', missingValue=None, inputCols=input_cols, outputCols=['f01','f02','f03','f04']).fit(dataset).transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "126db947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+------+---+---+---+---+\n",
      "|feature_1|feature_2|feature_3|feature_4|Class |f01|f02|f03|f04|\n",
      "+---------+---------+---------+---------+------+---+---+---+---+\n",
      "|        1|        1|        1|        0|     0|  0|  0|  0|  0|\n",
      "+---------+---------+---------+---------+------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.select([count(when(isnull(col), col)).alias(col) for col in dataset.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe0933",
   "metadata": {},
   "source": [
    "### Combine features in terms of vectors before feeding into the model along with scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b582d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "dataset = VectorAssembler(inputCols=['f01','f02','f03','f04'], outputCol='features', handleInvalid=\"keep\").transform(dataset)\n",
    "dataset = MinMaxScaler(min=0.0, max=1.0,inputCol='features', outputCol='features_sc').fit(dataset).transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bf704",
   "metadata": {},
   "source": [
    "### Show resulting dataset (first 20 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d02f7a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+------+------------------+------------------+--------+--------+--------------------+--------------------+\n",
      "|feature_1|feature_2|feature_3|feature_4|Class |               f01|               f02|     f03|     f04|            features|         features_sc|\n",
      "+---------+---------+---------+---------+------+------------------+------------------+--------+--------+--------------------+--------------------+\n",
      "|   3.6216|   8.6661|  -2.8073| -0.44699|   0.0|            3.6216|            8.6661| -2.8073|-0.44699|[3.6216,8.6661,-2...|[0.76900388695382...|\n",
      "|   4.5459|   8.1674|  -2.4586|  -1.4621|   0.0|            4.5459|            8.1674| -2.4586| -1.4621|[4.5459,8.1674,-2...|[0.83565901535310...|\n",
      "|    3.866|  -2.6383|   1.9242|  0.10645|   0.0|             3.866|           -2.6383|  1.9242| 0.10645|[3.866,-2.6383,1....|[0.78662859038429...|\n",
      "|   3.4566|   9.5228|  -4.0112|  -3.5944|   0.0|            3.4566|            9.5228| -4.0112| -3.5944|[3.4566,9.5228,-4...|[0.75710504871312...|\n",
      "|  0.32924|  -4.4552|   4.5718|  -0.9888|   0.0|           0.32924|           -4.4552|  4.5718| -0.9888|[0.32924,-4.4552,...|[0.53157807440740...|\n",
      "|     null|   9.6718|  -3.9606|  -3.1625|   0.0|0.4308653338439095|            9.6718| -3.9606| -3.1625|[0.43086533384390...|[0.53890670112598...|\n",
      "|   3.5912|   3.0129|  0.72888|  0.56421|   0.0|            3.5912|            3.0129| 0.72888| 0.56421|[3.5912,3.0129,0....|[0.76681161615068...|\n",
      "|   2.0922|    -6.81|   8.4636| -0.60216|   0.0|            2.0922|             -6.81|  8.4636|-0.60216|[2.0922,-6.81,8.4...|[0.65871247358818...|\n",
      "|   3.2032|   5.7588| -0.75345| -0.61251|   0.0|            3.2032|            5.7588|-0.75345|-0.61251|[3.2032,5.7588,-0...|[0.73883131774224...|\n",
      "|   1.5356|   9.1772|  -2.2718| -0.73535|   0.0|            1.5356|            9.1772| -2.2718|-0.73535|[1.5356,9.1772,-2...|[0.61857372592288...|\n",
      "|   1.2247|   8.7779|  -2.2135| -0.80647|   0.0|            1.2247|            8.7779| -2.2135|-0.80647|[1.2247,8.7779,-2...|[0.59615343011055...|\n",
      "|   3.9899|  -2.7066|   2.3946|  0.86291|   0.0|            3.9899|           -2.7066|  2.3946| 0.86291|[3.9899,-2.7066,2...|[0.79556353619049...|\n",
      "|   1.8993|   7.6625|  0.15394|  -3.1108|   0.0|            1.8993|            7.6625| 0.15394| -3.1108|[1.8993,7.6625,0....|[0.64480164997223...|\n",
      "|  -1.5768|   10.843|   2.5462|  -2.9362|   0.0|           -1.5768|            10.843|  2.5462| -2.9362|[-1.5768,10.843,2...|[0.39412557961765...|\n",
      "|    3.404|   8.7261|  -2.9915| -0.57242|   0.0|             3.404|            8.7261| -2.9915|-0.57242|[3.404,8.7261,-2....|[0.75331184331032...|\n",
      "|   4.6765|  -3.3895|   3.4896|   1.4771|   0.0|            4.6765|           -3.3895|  3.4896|  1.4771|[4.6765,-3.3895,3...|[0.84507712610605...|\n",
      "|   2.6719|   3.0646|  0.37158|  0.58619|   0.0|            2.6719|            3.0646| 0.37158| 0.58619|[2.6719,3.0646,0....|[0.70051705860718...|\n",
      "|  0.80355|   2.8473|   4.3439|   0.6017|   0.0|           0.80355|            2.8473|  4.3439|  0.6017|[0.80355,2.8473,4...|[0.56578254692829...|\n",
      "|   1.4479|     null|   8.3428|  -2.1086|   0.0|            1.4479|1.9273142826529555|  8.3428| -2.1086|[1.4479,1.9273142...|[0.61224931311251...|\n",
      "|   5.2423|  11.0272|   -4.353|  -4.1013|   0.0|            5.2423|           11.0272|  -4.353| -4.1013|[5.2423,11.0272,-...|[0.88587932414598...|\n",
      "+---------+---------+---------+---------+------+------------------+------------------+--------+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d2a93",
   "metadata": {},
   "source": [
    "### Split train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a12c4668",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df = dataset.select('Class ','features_sc').randomSplit([0.7,0.3],seed=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e613a",
   "metadata": {},
   "source": [
    "### Check shape of train & test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0f435aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(955, 2)\n",
      "(417, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((train_df.count(), len(train_df.columns))) , print((test_df.count(), len(test_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3579f742",
   "metadata": {},
   "source": [
    "### Create dense nueral network with 4 inputs(no. of features) + one hidden layer with 128 percepteron + 2 outputs as nuniques of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "412de1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "mlp = MultilayerPerceptronClassifier(featuresCol='features_sc', labelCol='Class ',\n",
    "                                    maxIter=100, seed=42, layers=(4, 128, 2), blockSize=8, stepSize=0.03, solver='l-bfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c22af6b",
   "metadata": {},
   "source": [
    "### Train & test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2a4623d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlp.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ae5f011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9ec3b",
   "metadata": {},
   "source": [
    "### Score the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "893319a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9976009848807263"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/05 03:40:33 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "22/04/05 03:40:33 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:919)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:154)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:262)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:169)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Class ',predictionCol='prediction',metricName='f1')\n",
    "evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347496e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
